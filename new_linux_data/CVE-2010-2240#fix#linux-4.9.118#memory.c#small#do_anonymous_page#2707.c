static int do_anonymous_page(struct fault_env *fe)
{
	struct vm_area_struct *vma = fe->vma;
	struct mem_cgroup *memcg;
	struct page *page;
	pte_t entry;

	if (vma->vm_flags & VM_SHARED)
		return VM_FAULT_SIGBUS;

	if (pte_alloc(vma->vm_mm, fe->pmd, fe->address))
		return VM_FAULT_OOM;

	if (unlikely(pmd_trans_unstable(fe->pmd)))
		return 0;

	if (!(fe->flags & FAULT_FLAG_WRITE) &&
			!mm_forbids_zeropage(vma->vm_mm)) {
		entry = pte_mkspecial(pfn_pte(my_zero_pfn(fe->address),
						vma->vm_page_prot));
		fe->pte = pte_offset_map_lock(vma->vm_mm, fe->pmd, fe->address,
				&fe->ptl);
		if (!pte_none(*fe->pte))
			goto unlock;
		if (userfaultfd_missing(vma)) {
			pte_unmap_unlock(fe->pte, fe->ptl);
			return handle_userfault(fe, VM_UFFD_MISSING);
		}
		goto setpte;
	}

	if (unlikely(anon_vma_prepare(vma)))
		goto oom;
	page = alloc_zeroed_user_highpage_movable(vma, fe->address);
	if (!page)
		goto oom;

	if (mem_cgroup_try_charge(page, vma->vm_mm, GFP_KERNEL, &memcg, false))
		goto oom_free_page;

	__SetPageUptodate(page);

	entry = mk_pte(page, vma->vm_page_prot);
	if (vma->vm_flags & VM_WRITE)
		entry = pte_mkwrite(pte_mkdirty(entry));

	fe->pte = pte_offset_map_lock(vma->vm_mm, fe->pmd, fe->address,
			&fe->ptl);
	if (!pte_none(*fe->pte))
		goto release;

	if (userfaultfd_missing(vma)) {
		pte_unmap_unlock(fe->pte, fe->ptl);
		mem_cgroup_cancel_charge(page, memcg, false);
		put_page(page);
		return handle_userfault(fe, VM_UFFD_MISSING);
	}

	inc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);
	page_add_new_anon_rmap(page, vma, fe->address, false);
	mem_cgroup_commit_charge(page, memcg, false, false);
	lru_cache_add_active_or_unevictable(page, vma);
setpte:
	set_pte_at(vma->vm_mm, fe->address, fe->pte, entry);

	update_mmu_cache(vma, fe->address, fe->pte);
unlock:
	pte_unmap_unlock(fe->pte, fe->ptl);
	return 0;
release:
	mem_cgroup_cancel_charge(page, memcg, false);
	put_page(page);
	goto unlock;
oom_free_page:
	put_page(page);
oom:
	return VM_FAULT_OOM;
}
