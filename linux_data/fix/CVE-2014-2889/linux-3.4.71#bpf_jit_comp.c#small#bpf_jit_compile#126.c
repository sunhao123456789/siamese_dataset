void bpf_jit_compile(struct sk_filter *fp)
{
	u8 temp[64];
	u8 *prog;
	unsigned int proglen, oldproglen = 0;
	int ilen, i;
	int t_offset, f_offset;
	u8 t_op, f_op, seen = 0, pass;
	u8 *image = NULL;
	u8 *func;
	unsigned int *addrs;
	const struct sock_filter *filter = fp->insns;
	int flen = fp->len;

	if (!bpf_jit_enable)
		return;

	addrs = kmalloc(flen * sizeof(*addrs), GFP_KERNEL);
	if (addrs == NULL)
		return;

	for (proglen = 0, i = 0; i < flen; i++) {
		proglen += 64;
		addrs[i] = proglen;
	}

	for (pass = 0; pass < 10; pass++) {
		u8 seen_or_pass0 = (pass == 0) ? (SEEN_XREG | SEEN_DATAREF | SEEN_MEM) : seen;epilogue for trivial filters (RET something) */
		proglen = 0;
		prog = temp;

		if (seen_or_pass0) {
			if (seen_or_pass0 & (SEEN_XREG | SEEN_DATAREF))
			if (seen_or_pass0 & SEEN_XREG)

			if (seen_or_pass0 & SEEN_DATAREF) {
				if (offsetof(struct sk_buff, len) <= 127)
					EMIT4(0x44, 0x8b, 0x4f, offsetof(struct sk_buff, len));
				else {
					EMIT3(0x44, 0x8b, 0x8f);
					EMIT(offsetof(struct sk_buff, len), 4);
				}
				if (is_imm8(offsetof(struct sk_buff, data_len)))
					EMIT4(0x44, 0x2b, 0x4f, offsetof(struct sk_buff, data_len));
				else {
					EMIT3(0x44, 0x2b, 0x8f);
					EMIT(offsetof(struct sk_buff, data_len), 4);
				}

				if (is_imm8(offsetof(struct sk_buff, data)))
					EMIT4(0x4c, 0x8b, 0x47, offsetof(struct sk_buff, data));
				else {
					EMIT3(0x4c, 0x8b, 0x87);
					EMIT(offsetof(struct sk_buff, data), 4);
				}
			}
		}

		switch (filter[0].code) {
		case BPF_S_RET_K:
		case BPF_S_LD_W_LEN:
		case BPF_S_ANC_PROTOCOL:
		case BPF_S_ANC_IFINDEX:
		case BPF_S_ANC_MARK:
		case BPF_S_ANC_RXHASH:
		case BPF_S_ANC_CPU:
		case BPF_S_ANC_QUEUE:
		case BPF_S_LD_W_ABS:
		case BPF_S_LD_H_ABS:
		case BPF_S_LD_B_ABS:
			break;
		default:
		}

		for (i = 0; i < flen; i++) {
			unsigned int K = filter[i].k;

			switch (filter[i].code) {
				seen |= SEEN_XREG;
				break;
				if (!K)
					break;
				if (is_imm8(K))
				else
				break;
				seen |= SEEN_XREG;
				break;
				if (!K)
					break;
				if (is_imm8(K))
				else
				break;
				seen |= SEEN_XREG;
				break;
				if (is_imm8(K))
				else {
					EMIT(K, 4);
				}
				break;= X; */
				seen |= SEEN_XREG;
				if (pc_ret0 > 0) {
					EMIT_COND_JMP(X86_JE, addrs[pc_ret0 - 1] -
								(addrs[i] - 4));
				} else {
					EMIT_COND_JMP(X86_JNE, 2 + 5);
					CLEAR_A();
				}
				break;
				EMIT(K, 4);
				break;
			case BPF_S_ALU_AND_X:
				seen |= SEEN_XREG;
				break;
			case BPF_S_ALU_AND_K:
				if (K >= 0xFFFFFF00) {
				} else if (K >= 0xFFFF0000) {
					EMIT(K, 2);
				} else {
				}
				break;
			case BPF_S_ALU_OR_X:
				seen |= SEEN_XREG;
				break;
			case BPF_S_ALU_OR_K:
				if (is_imm8(K))
				else
				break;
				seen |= SEEN_XREG;
				break;
			case BPF_S_ALU_LSH_K:
				if (K == 0)
					break;
				else if (K == 1)
				else
					EMIT3(0xc1, 0xe0, K);
				break;
				seen |= SEEN_XREG;
				break;
				if (K == 0)
					break;
				else if (K == 1)
				else
					EMIT3(0xc1, 0xe8, K);
				break;
			case BPF_S_ALU_NEG:
				break;
			case BPF_S_RET_K:
				if (!K) {
					if (pc_ret0 == -1)
						pc_ret0 = i;
					CLEAR_A();
				} else {
				}
			case BPF_S_RET_A:
				if (seen_or_pass0) {
					if (i != flen - 1) {
						EMIT_JMP(cleanup_addr - addrs[i]);
						break;
					}
					if (seen_or_pass0 & SEEN_XREG)
				}
				break;
				seen |= SEEN_XREG;
				break;
				seen |= SEEN_XREG;
				break;
				if (!K)
					CLEAR_A();
				else
				break;
				seen |= SEEN_XREG;
				if (!K)
					CLEAR_X();
				else
				break;
				seen |= SEEN_MEM;
				EMIT3(0x8b, 0x45, 0xf0 - K*4);
				break;
				seen |= SEEN_XREG | SEEN_MEM;
				EMIT3(0x8b, 0x5d, 0xf0 - K*4);
				break;
				seen |= SEEN_MEM;
				EMIT3(0x89, 0x45, 0xf0 - K*4);
				break;
				seen |= SEEN_XREG | SEEN_MEM;
				EMIT3(0x89, 0x5d, 0xf0 - K*4);
				break;
				BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, len) != 4);
				if (is_imm8(offsetof(struct sk_buff, len)))
					EMIT3(0x8b, 0x47, offsetof(struct sk_buff, len));
				else {
					EMIT2(0x8b, 0x87);
					EMIT(offsetof(struct sk_buff, len), 4);
				}
				break;
				seen |= SEEN_XREG;
				if (is_imm8(offsetof(struct sk_buff, len)))
					EMIT3(0x8b, 0x5f, offsetof(struct sk_buff, len));
				else {
					EMIT2(0x8b, 0x9f);
					EMIT(offsetof(struct sk_buff, len), 4);
				}
				break;
				BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, protocol) != 2);
				if (is_imm8(offsetof(struct sk_buff, protocol))) {
					EMIT4(0x0f, 0xb7, 0x47, offsetof(struct sk_buff, protocol));
				} else {
					EMIT(offsetof(struct sk_buff, protocol), 4);
				}
				break;
			case BPF_S_ANC_IFINDEX:
				if (is_imm8(offsetof(struct sk_buff, dev))) {
					EMIT4(0x48, 0x8b, 0x47, offsetof(struct sk_buff, dev));
				} else {
					EMIT(offsetof(struct sk_buff, dev), 4);
				}
				EMIT_COND_JMP(X86_JE, cleanup_addr - (addrs[i] - 6));
				BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, ifindex) != 4);
				EMIT(offsetof(struct net_device, ifindex), 4);
				break;
			case BPF_S_ANC_MARK:
				BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);
				if (is_imm8(offsetof(struct sk_buff, mark))) {
					EMIT3(0x8b, 0x47, offsetof(struct sk_buff, mark));
				} else {
					EMIT2(0x8b, 0x87);
					EMIT(offsetof(struct sk_buff, mark), 4);
				}
				break;
			case BPF_S_ANC_RXHASH:
				BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, rxhash) != 4);
				if (is_imm8(offsetof(struct sk_buff, rxhash))) {
					EMIT3(0x8b, 0x47, offsetof(struct sk_buff, rxhash));
				} else {
					EMIT2(0x8b, 0x87);
					EMIT(offsetof(struct sk_buff, rxhash), 4);
				}
				break;
			case BPF_S_ANC_QUEUE:
				BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, queue_mapping) != 2);
				if (is_imm8(offsetof(struct sk_buff, queue_mapping))) {
					EMIT4(0x0f, 0xb7, 0x47, offsetof(struct sk_buff, queue_mapping));
				} else {
					EMIT(offsetof(struct sk_buff, queue_mapping), 4);
				}
				break;
			case BPF_S_ANC_CPU:
#ifdef CONFIG_SMP
#else
				CLEAR_A();
#endif
				break;
			case BPF_S_LD_W_ABS:
				func = CHOOSE_LOAD_FUNC(K, sk_load_word);
common_load:			seen |= SEEN_DATAREF;
				t_offset = func - (image + addrs[i]);
				break;
			case BPF_S_LD_H_ABS:
				func = CHOOSE_LOAD_FUNC(K, sk_load_half);
				goto common_load;
			case BPF_S_LD_B_ABS:
				func = CHOOSE_LOAD_FUNC(K, sk_load_byte);
				goto common_load;
			case BPF_S_LDX_B_MSH:
				func = CHOOSE_LOAD_FUNC(K, sk_load_byte_msh);
				seen |= SEEN_DATAREF | SEEN_XREG;
				t_offset = func - (image + addrs[i]);
				break;
			case BPF_S_LD_W_IND:
				func = sk_load_word;
common_load_ind:		seen |= SEEN_DATAREF | SEEN_XREG;
				t_offset = func - (image + addrs[i]);
				if (K) {
					if (is_imm8(K)) {
					} else {
						EMIT(K, 4);
					}
				} else {
				}
				break;
			case BPF_S_LD_H_IND:
				func = sk_load_half;
				goto common_load_ind;
			case BPF_S_LD_B_IND:
				func = sk_load_byte;
				goto common_load_ind;
			case BPF_S_JMP_JA:
				t_offset = addrs[i + K] - addrs[i];
				EMIT_JMP(t_offset);
				break;
			COND_SEL(BPF_S_JMP_JGT_K, X86_JA, X86_JBE);
			COND_SEL(BPF_S_JMP_JGE_K, X86_JAE, X86_JB);
			COND_SEL(BPF_S_JMP_JEQ_K, X86_JE, X86_JNE);
			COND_SEL(BPF_S_JMP_JSET_K,X86_JNE, X86_JE);
			COND_SEL(BPF_S_JMP_JGT_X, X86_JA, X86_JBE);
			COND_SEL(BPF_S_JMP_JGE_X, X86_JAE, X86_JB);
			COND_SEL(BPF_S_JMP_JEQ_X, X86_JE, X86_JNE);
			COND_SEL(BPF_S_JMP_JSET_X,X86_JNE, X86_JE);

cond_branch:			f_offset = addrs[i + filter[i].jf] - addrs[i];
				t_offset = addrs[i + filter[i].jt] - addrs[i];

				if (filter[i].jt == filter[i].jf) {
					EMIT_JMP(t_offset);
					break;
				}

				switch (filter[i].code) {
				case BPF_S_JMP_JGT_X:
				case BPF_S_JMP_JGE_X:
				case BPF_S_JMP_JEQ_X:
					seen |= SEEN_XREG;
					break;
				case BPF_S_JMP_JSET_X:
					seen |= SEEN_XREG;
					break;
				case BPF_S_JMP_JEQ_K:
					if (K == 0) {
						break;
					}
				case BPF_S_JMP_JGT_K:
				case BPF_S_JMP_JGE_K:
					if (K <= 127)
					else
					break;
				case BPF_S_JMP_JSET_K:
					if (K <= 0xFF)
					else if (!(K & 0xFFFF00FF))
					else if (K <= 0xFFFF) {
						EMIT(K, 2);
					} else {
					}
					break;
				}
				if (filter[i].jt != 0) {
					if (filter[i].jf && f_offset)
						t_offset += is_near(f_offset) ? 2 : 5;
					EMIT_COND_JMP(t_op, t_offset);
					if (filter[i].jf)
						EMIT_JMP(f_offset);
					break;
				}
				EMIT_COND_JMP(f_op, f_offset);
				break;
			default:
				goto out;
			}
			ilen = prog - temp;
			if (image) {
				if (unlikely(proglen + ilen > oldproglen)) {
					pr_err("bpb_jit_compile fatal error\n");
					kfree(addrs);
					module_free(NULL, image);
					return;
				}
				memcpy(image + proglen, temp, ilen);
			}
			proglen += ilen;
			addrs[i] = proglen;
			prog = temp;
		}
		if (seen_or_pass0)
		if (seen_or_pass0 & SEEN_XREG)

		if (image) {
			if (proglen != oldproglen)
				pr_err("bpb_jit_compile proglen=%u != oldproglen=%u\n", proglen, oldproglen);
			break;
		}
		if (proglen == oldproglen) {
			image = module_alloc(max_t(unsigned int,
						   proglen,
						   sizeof(struct work_struct)));
			if (!image)
				goto out;
		}
		oldproglen = proglen;
	}
	if (bpf_jit_enable > 1)
		pr_err("flen=%d proglen=%u pass=%d image=%p\n",
		       flen, proglen, pass, image);

	if (image) {
		if (bpf_jit_enable > 1)
			print_hex_dump(KERN_ERR, "JIT code: ", DUMP_PREFIX_ADDRESS,
				       16, 1, image, proglen, false);

		bpf_flush_icache(image, image + proglen);

		fp->bpf_func = (void *)image;
	}
out:
	kfree(addrs);
	return;
}
